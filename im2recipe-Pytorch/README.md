# im2recipe

## Setup
```bash
conda env create -f environment.yaml
conda activate im2recipe
pip install torchwordemb
```

## 1. scripts/mk\_dataset.py

You don't need to run this if you already have the Output files.
Note that about 53GB of RAM is required to run this.

```bash
cd scripts
python mk_dataset.py --vocab ../data/text/vocab.txt --sthdir ../data
```

### Input
* `data/recipe1M/layer1.json`: ingredients, url, partition, title, id, instructions
* `data/recipe1M/layer2.json`: id, image urls
* `data/recipe1M/det_ingrs.json`: id, ingredients, validity
* `data/classes1M.pkl`: Title classification (download it, or run `python bigrams --crtbgrs`)
* `data/encs_test_1024.t7`: Skip-thought instructions, test
* `data/encs_train_1024.t7`: Skip-thought instructions, train
* `data/encs_val_1024.t7`: Skip-thought instructions, val
* `data/text/vocab.txt`: word2vec text (download vocab.bin.gz, and run `python get_vocab.py ../data/vocab.bin`)

### Output

* `data/train_keys.pkl`: List of recipe ids, train
* `data/val_keys.pkl`: List of recipe ids, val
* `data/test_keys.pkl`: List of recipe ids, test
* `data/train_lmdb`: Recipe id to ingrs, intrs, classes, and imgs mapping, train
* `data/val_lmdb`: Recipe id to ingrs, intrs, classes, and imgs mapping, val
* `data/test_lmdb`: Recipe id to ingrs, intrs, classes, and imgs mapping, test

## 2. train.py

Each epoch trains 238,459 entries, and validation runs 51,129 entries at valfreq.
The default setting causes out of memory errors with 12GB VRAM.
Reducing the batch size seems to have the largest impact, and reducing the number of workers also helps.

```bash
# epochs: 100, valfreq: 20, batch size: 160, workers: 30
python train.py

# epochs: 100, valfreq: 1, batch size: 25, workers: 8
python train.py --valfreq=1 --batch_size=25 --workers=8
```

### Input
* `data/train_keys.pkl`: List of recipe ids, train
* `data/train_lmdb`: Recipe id to ingrs, intrs, classes, and imgs mapping, train
* `data/val_keys.pkl`: List of recipe ids, val
* `data/val_lmdb`: Recipe id to ingrs, intrs, classes, and imgs mapping, val
* `data/images/train`: Recipe images, train
* `data/images/val`: Recipe images, val
* `data/text/vocab.bin`: word2vec binary

### Output
* `snapshots/model_eXXX_v-Y.YYY.pth.tar`: X=epoch, Y=best\_val

## 3. test.py

It processes 51,334 entries and calculates the cosine loss of a model.
It also saves embeddings generated by the model.

```bash
# batch size: 160, workers: 30
python test.py --model_path=snapshots/model_eXXX_v-Y.YYY.pth.tar

# batch size: 25, workers: 8
python test.py --model_path=snapshots/model_eXXX_v-Y.YYY.pth.tar --batch_size=25 --workers=8
```

You can run `cd scripts; python rank.py --path_results=../results` to also see the median rank loss, using the embeddings.

### Input
* `snapshots/model_eXXX_v-Y.YYY.pth.tar`: A snapshot of a model you trained
* `data/test_keys.pkl`: List of recipe ids, test
* `data/test_lmdb`: Recipe id to ingrs, intrs, classes, and imgs mapping, test
* `data/images/test`: Recipe images, test

### Output
* `results/img_embeds.pkl`: Image embeddings of the test dataset
* `results/rec_embeds.pkl`: Recipe embeddings of the test dataset
* `results/img_ids.pkl`: Image ids of the image embeddings
* `results/rec_ids.pkl`: Recipe ids of the image/recipe embeddings

## 4. predict.py

Using the embeddings created by test.py, you can query recipes from the test dataset with an image.
Note that you must use the same model as the one used in test.py. Otherwise the result wouldn't make any sense.

```
python predict.py --test_image_path=[image].jpg --model_path=snapshots/model_eXXX_v-Y.YYY.pth.tar
```

### Input
* `[image].jpg`: An image of your choice
* `snapshots/model_eXXX_v-Y.YYY.pth.tar`: A snapshot of a model you trained
* `results/rec_embeds.pkl`: Recipe embeddings of the test dataset
* `results/rec_ids.pkl`: Recipe ids of the image/recipe embeddings
* `data/recipe1M/layer1.json`: ingredients, url, partition, title, id, instructions
